{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwi9mzFPPDv7"
      },
      "source": [
        "# Identifying Entities in Healthcare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import os\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install pycrf\n",
        "!pip install sklearn-crfsuite\n",
        "\n",
        "import spacy\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import metrics\n",
        "import pandas as pd\n",
        "\n",
        "model = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Read train and test data\n",
        "with open('train_sent', 'r') as f:\n",
        "    train_words = f.readlines()\n",
        "\n",
        "with open('train_label', 'r') as f:\n",
        "    train_labels_by_word = f.readlines()\n",
        "\n",
        "with open('test_sent', 'r') as f:\n",
        "    test_words = f.readlines()\n",
        "\n",
        "with open('test_label', 'r') as f:\n",
        "    test_labels_by_word = f.readlines()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Check if word and label counts match\n",
        "print(f\"Train words: {len(train_words)}, Train labels: {len(train_labels_by_word)}\")\n",
        "print(f\"Test words: {len(test_words)}, Test labels: {len(test_labels_by_word)}\")\n",
        "\n",
        "# Convert token lists to sentences\n",
        "def convert_to_sentences(dataset):\n",
        "    sentences = []\n",
        "    sentence = \"\"\n",
        "    for entity in dataset:\n",
        "        if entity != '\\n':\n",
        "            sentence += entity.strip() + \" \"\n",
        "        else:\n",
        "            sentences.append(sentence.strip())\n",
        "            sentence = \"\"\n",
        "    return sentences\n",
        "\n",
        "train_sentences = convert_to_sentences(train_words)\n",
        "train_labels = convert_to_sentences(train_labels_by_word)\n",
        "test_sentences = convert_to_sentences(test_words)\n",
        "test_labels = convert_to_sentences(test_labels_by_word)\n",
        "\n",
        "print(\"First 5 training sentences and labels:\")\n",
        "for i in range(5):\n",
        "    print(train_sentences[i], \"\\n\", train_labels[i], \"\\n\")\n",
        "\n",
        "print(\"First 5 test sentences and labels:\")\n",
        "for i in range(5):\n",
        "    print(test_sentences[i], \"\\n\", test_labels[i], \"\\n\")\n",
        "\n",
        "# Sentence and label counts\n",
        "print(f\"Train sentences: {len(train_sentences)}, Test sentences: {len(test_sentences)}\")\n",
        "print(f\"Train labels: {len(train_labels)}, Test labels: {len(test_labels)}\")\n",
        "\n",
        "# Combine datasets for analysis\n",
        "combined = train_sentences + test_sentences\n",
        "print(f\"Combined sentences: {len(combined)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Extract NOUN and PROPN tokens\n",
        "noun_propn = []\n",
        "pos_tag = []\n",
        "\n",
        "for sent in combined:\n",
        "    for token in model(sent):\n",
        "        if token.pos_ in ['NOUN', 'PROPN']:\n",
        "            noun_propn.append(token.text)\n",
        "            pos_tag.append(token.pos_)\n",
        "\n",
        "print(f\"NOUN/PROPN tokens: {len(noun_propn)}\")\n",
        "\n",
        "noun_pos = pd.DataFrame({\"NOUN_PROPN\": noun_propn, \"POS_tag\": pos_tag})\n",
        "print(\"Top 25 NOUN/PROPN tokens:\")\n",
        "print(noun_pos[\"NOUN_PROPN\"].value_counts().head(25))\n",
        "\n",
        "# POS tagging example\n",
        "sentence = train_sentences[1]\n",
        "words = sentence.split()\n",
        "position = 2\n",
        "word = words[position]\n",
        "\n",
        "print(f\"Sentence: {sentence}\")\n",
        "print(f\"POS tag (isolated): {model(word)[0].pos_}\")\n",
        "\n",
        "print(\"POS tags (contextual):\")\n",
        "for token in model(sentence):\n",
        "    print(f\"{token.text} -- {token.pos_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Function for contextual POS tagging\n",
        "def contextual_pos_tagger(sent_list, position):\n",
        "    sentence = \" \".join(sent_list)\n",
        "    for i, token in enumerate(model(sentence)):\n",
        "        if i == position:\n",
        "            return token.pos_\n",
        "\n",
        "# Feature extraction for one word\n",
        "def get_features_for_one_word(sent_list, position):\n",
        "    word = sent_list[position]\n",
        "    features = [\n",
        "        'word.lower=' + word.lower(),\n",
        "        'word.postag=' + contextual_pos_tagger(sent_list, position),\n",
        "        'word[-3:]=' + word[-3:],\n",
        "        'word[-2:]=' + word[-2:],\n",
        "        'word.isupper=%s' % word.isupper(),\n",
        "        'word.isdigit=%s' % word.isdigit(),\n",
        "        'word.startsWithCapital=%s' % word[0].isupper()\n",
        "    ]\n",
        "    if position > 0:\n",
        "        prev_word = sent_list[position-1]\n",
        "        features.extend([\n",
        "            'prev_word.lower=' + prev_word.lower(),\n",
        "            'prev_word.postag=' + contextual_pos_tagger(sent_list, position-1),\n",
        "            'prev_word.isupper=%s' % prev_word.isupper(),\n",
        "            'prev_word.isdigit=%s' % prev_word.isdigit(),\n",
        "            'prev_word.startsWithCapital=%s' % prev_word[0].isupper()\n",
        "        ])\n",
        "    else:\n",
        "        features.append('BEG')\n",
        "    if position == len(sent_list)-1:\n",
        "        features.append('END')\n",
        "    return features\n",
        "\n",
        "# Feature extraction for one sentence\n",
        "def get_features_for_one_sentence(sentence):\n",
        "    words = sentence.split()\n",
        "    return [get_features_for_one_word(words, i) for i in range(len(words))]\n",
        "\n",
        "# Label extraction for one sentence\n",
        "def get_labels_for_one_sentence(labels):\n",
        "    return labels.split()\n",
        "\n",
        "# Check feature and label extraction\n",
        "example_sentence = train_sentences[5]\n",
        "print(example_sentence)\n",
        "print(get_features_for_one_sentence(example_sentence)[:2])\n",
        "\n",
        "example_labels = get_labels_for_one_sentence(train_labels[5])\n",
        "print(example_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Extract features and labels for train and test sets\n",
        "X_train = [get_features_for_one_sentence(s) for s in train_sentences]\n",
        "X_test = [get_features_for_one_sentence(s) for s in test_sentences]\n",
        "Y_train = [get_labels_for_one_sentence(l) for l in train_labels]\n",
        "Y_test = [get_labels_for_one_sentence(l) for l in test_labels]\n",
        "\n",
        "# Build and train CRF model\n",
        "crf = sklearn_crfsuite.CRF(max_iterations=300)\n",
        "crf.fit(X_train, Y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "Y_pred = crf.predict(X_test)\n",
        "print(\"F1 score:\", metrics.flat_f1_score(Y_test, Y_pred, average='weighted'))\n",
        "\n",
        "# Example test sentence and labels\n",
        "print(f\"Sentence: {test_sentences[13]}\")\n",
        "print(f\"Actual: {Y_test[13]}\")\n",
        "print(f\"Predicted: {Y_pred[13]}\")\n",
        "print(X_test[13])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Extract diseases and treatments\n",
        "disease_treatment = {}\n",
        "for i in range(len(Y_pred)):\n",
        "    diseases = []\n",
        "    treatments = []\n",
        "    for j, label in enumerate(Y_pred[i]):\n",
        "        if label == 'D':\n",
        "            diseases.append(X_test[i][j][0].split('=')[1])\n",
        "        elif label == 'T':\n",
        "            treatments.append(X_test[i][j][0].split('=')[1])\n",
        "    for disease in diseases:\n",
        "        if disease in disease_treatment:\n",
        "            disease_treatment[disease].extend(treatments)\n",
        "        else:\n",
        "            disease_treatment[disease] = treatments\n",
        "\n",
        "# Clean dictionary\n",
        "cleaned_dict = {k: v for k, v in disease_treatment.items() if v}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Convert to dataframe\n",
        "cleaned_df = pd.DataFrame({\"Disease\": cleaned_dict.keys(), \"Treatments\": cleaned_dict.values()})\n",
        "print(cleaned_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Search treatments for a specific disease\n",
        "search_item = 'hereditary retinoblastoma'\n",
        "treatments = cleaned_dict.get(search_item, [])\n",
        "print(f\"Treatments for '{search_item}': {', '.join(treatments)}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
